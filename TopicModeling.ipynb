{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling (EM algorithm and Gibbs Sampling)\n",
    "\n",
    "## Probabilistic Latent Semantic Analysis with the EM Algorithm\n",
    "\n",
    "In this homework you will exercise your expertise on the EM algorithm and Gibbs sampling to apply it to Probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation.\n",
    "\n",
    "### Part I: The pLSAModel\n",
    "\n",
    "Recall that our data model for pLSA will consist of a set of documents $D$, and each document is modeled as a bag of words over dictionary $W$, we denote $x_{w,d}$ as the number of times word $w \\in W$ appears in document $d \\in D$.\n",
    "\n",
    "#### Warmup: A simple multinomial model\n",
    "\n",
    "Before we introduce the concept of topics, let's build a simple model based on frequency of word occurences to get used to Maximum Likelihood Estimation for multinomial distributions. Specifically, letting $n_d$ be the number of words in document $d$, then we model each document $d$ as $n_d$ draws from a Multinomial distribution with parameters $\\theta_{1,d},\\ldots,\\theta_{W,d}$ with $\\theta_{w,d}$ the probability of drawing word $w$ in document $d$. Note that $\\theta_{w,d} \\geq 0$ for all $w \\in W$, and $\\sum_w \\theta_{w,d} = 1$.\n",
    "\n",
    "With this model in place, the probability of observing the set of words in document $d$ is given by\n",
    "\n",
    "$$\n",
    "Pr(d|\\theta_d) \\varpropto \\prod_{w=1}^{W} \\theta_{w,d}^{x_{w,d}}\n",
    "$$\n",
    "\n",
    "where $\\theta_d$ collects parameters $\\{\\theta_{1,d},\\ldots,\\theta_{W,d}\\}$.\n",
    "\n",
    "**Problem 1**: Prove that Maximum Likelihood Estimates (MLE) are given by \n",
    "\n",
    "$$\\hat{\\theta}_{w,d} = \\frac{x_{w,d}}{n_d}$$, \n",
    "\n",
    "that is, the number of times word $w$ appears in document $d$ divided by the total number of words in document $d$.\n",
    "\n",
    "_Hints_:\n",
    "\n",
    "- Write MLE estimation problem as a _constrained_ maximization problem\n",
    "\n",
    "- Write out the Lagrangian $L(\\theta_d,\\lambda, \\nu)$ (see lecture slides) for this maximization problem.\n",
    "\n",
    "- Use optimality conditions from lecture to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1 Answer**\n",
    "\n",
    "The log likelihood of the model is given by \n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\theta_d) = \\sum_{w=1}^W x_{w,d} \\log \\theta_{w,d}\n",
    "$$\n",
    "\n",
    "So, the MLE problem in standard form is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{\\theta_d} & -\\sum_{w=1}^W x_{w,d} \\log \\theta_{w,d} \\\\\n",
    "\\textrm{s.t.} & -\\theta_{w,d} \\leq 0 \\; \\forall w \\\\\n",
    "{} & \\sum_{w=1}^W \\theta_{w,d} = 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "The Lagrangian of the problem is then\n",
    "\n",
    "$$\n",
    "L(\\theta_d, \\lambda, \\nu) = -\\sum_{w=1}^W x_{w,d} \\log \\theta_{w,d} + \\sum_{w,d} \\lambda_{w,d} (-\\theta_{w,d}) + \\nu(\\sum_{w=1}^W \\theta_{w,d} - 1)\n",
    "$$\n",
    "\n",
    "To solve, we use optimality conditions. First, the gradient of Lagrangian wrt to parameter should be zero:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial \\theta_{w,d}} = \\frac{-x_{w,d}}{\\theta_{w,d}} - \\lambda_{w,d} + \\nu \\overset{\\mathrm{set}}{=} 0 & \\Rightarrow \\\\\n",
    "\\theta_{w,d} = \\frac{- x_{w,d}}{\\lambda_{w,d} - \\nu} & {}\n",
    "\\end{eqnarray}\n",
    "\n",
    "From complementarity and dual feasibility we can set $\\lambda_{w,d}=0$.\n",
    "\n",
    "From primal feasibility we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{w,d} \\theta_{w,d} = 1 & \\Rightarrow \\\\\n",
    "\\frac{\\sum_{w,d} x_{w,d}}{\\nu} = 1 & \\Rightarrow \\\\\n",
    "\\nu = \\sum_{w,d} x_{w,d} & \\Rightarrow \\\\\n",
    "\\nu = n_d & \\Rightarrow \\\\\n",
    "\\theta_{w,d} = \\frac{x_{w,d}}{n_d} & {}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fully observed topic model\n",
    "\n",
    "Let's introduce topics now. Instead of modeling each document as $d \\sim \\mathrm{Mult}(\\{\\theta_{1,d},\\ldots,\\theta_{W,d}\\})$ over words, we model each document as a distribution over $T$ _topics_ as $d \\sim \\mathrm{Mult}(\\{p_{1,d},\\ldots,p_{T,d}\\})$. In turn, each topic $t=1,\\ldots,T$ is modeled as a distribution $t \\sim \\mathrm{Mult}(\\{\\theta_{1,t},\\ldots,\\theta_{W,t}\\})$ over words. Note that the topics are shared across documents in dataset.\n",
    "\n",
    "In pLSA, we learn topic distributions from observations by a soft assignment of each word occurence to topics using the EM algorithm. We will denote these _latent_ word-topic assignments as $\\Delta_{w,d,t}$ to represent the number of times word $w$ was assigned to topic $t$ in document $d$.\n",
    "\n",
    "Of course, we do not observe any of these latent word-topic assignments. However, it is helpful to think of the fully observed case to develop the EM algorithm. \n",
    "\n",
    "Assuming we observe word occurences $x_{w,d}$ and latent word-topic assignments $\\Delta_{w,d,t}$ such that $\\sum_t \\Delta_{w,d,t} = x_{w,d}$ the full data probability is given by\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(D|\\{p_d\\},\\{\\theta_t\\}) = \\prod_{d=1}^D \\prod_{w=1}^{W} \\prod_{t=1}^T p_{t,d}^{\\Delta_{w,d,t}}\\theta_{w,t}^{\\Delta_{w,d,t}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Problem 2**: Prove that MLEs are given by\n",
    "\n",
    "$$\n",
    "\\hat{p}_{t,d} = \\frac{\\sum_{w=1}^W \\Delta_{w,d,t}}{\\sum_{t=1}^T \\sum_{w=1}^W \\Delta_{w,d,t}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{w,t} = \\frac{\\sum_{d=1}^D \\Delta_{w,d,t}}{\\sum_{w=1}^W \\sum_{d=1}^D \\Delta_{w,d,t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2 Answer**\n",
    "\n",
    "The log likelihood of the model is given by \n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\theta_d) = \\sum_{d=1}^D\\sum_{w=1}^W \\sum_{t=1}^T \\Delta_{w,d,t} \\log p_{t,d} + \\Delta_{w,d,t} \\log \\theta_{w,t}\n",
    "$$\n",
    "\n",
    "So, the MLE problem in standard form is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{p_d,\\theta_t} & -\\sum_{d=1}^D\\sum_{w=1}^W\\sum_{t=1}^T \\Delta_{w,d,t} p_{t,d} + \\Delta_{w,d,t} \\log \\theta_{w,t} \\\\\n",
    "\\textrm{s.t.} & -p_{t,d} \\leq 0\\; \\forall t,d \\\\\n",
    "{} & -\\theta_{w,t} \\leq 0 \\; \\forall w,t \\\\\n",
    "{} & \\sum_{t=1}^T p_{t,d} = 1 \\; \\forall d \\\\\n",
    "{} & \\sum_{w=1}^W \\theta_{w,t} = 1 \\; \\forall t\n",
    "\\end{eqnarray}\n",
    "\n",
    "The Lagrangian of the problem is then\n",
    "\n",
    "$$\n",
    "L(p_{d},\\theta_{t}, \\lambda^p_d, \\lambda^{\\theta}_t, \\nu^d, \\nu^t) = -\\sum_{d=1}^D\\sum_{w=1}^W \\sum_{t=1}^T \\Delta_{w,d,t} \\log p_{t,d} + \\Delta_{w,d,t} \\log \\theta_{w,t} + \\sum_{t,d} \\lambda^p_{t,d} (-p_{t,d}) + \\sum_{w,t} \\lambda^{\\theta}_{w,t} (-\\theta_{w,t}) + \\sum_{d=1}^D \\nu^p_d(\\sum_{t=1}^T p_{t,d}) + \\sum_{t=1}^T \\nu^{\\theta}_t (\\sum_{w=1}^W \\theta_{w,t} - 1)\n",
    "$$\n",
    "To solve for $p_{t,d}$, we use optimality conditions. First, the gradient of Lagrangian wrt to parameter should be zero:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial p{t,d}} = \\frac{-\\sum_{w=1}^W \\Delta_{w,d,t}}{p_{t,d}} - \\lambda^p_{t,d} + \\nu^p \\overset{\\mathrm{set}}{=} 0 & \\Rightarrow \\\\\n",
    "p_{t,d} = \\frac{- \\sum_{w=1}^W \\Delta_{w,d,t}}{\\lambda^p_{t,d} - \\nu^p_d} & {}\n",
    "\\end{eqnarray}\n",
    "\n",
    "From complementarity and dual feasibility we can set $\\lambda^p_{t,d}=0$.\n",
    "\n",
    "From primal feasibility we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{t} p_{t,d} = 1 & \\Rightarrow \\\\\n",
    "\\frac{\\sum_{t} \\sum_{w=1}^W \\Delta_{w,d,t}}{\\nu^p_d} = 1 & \\Rightarrow \\\\\n",
    "\\nu^p_d = \\sum_t \\sum_{w} \\Delta_{w,d,t} & \\Rightarrow \\\\\n",
    "p_{t,d} = \\frac{\\sum_w \\Delta_{w,d,t}}{\\sum_t \\sum_w \\Delta_{w,d,t}} & {}\n",
    "\\end{eqnarray}\n",
    "\n",
    "To solve for $\\theta_{w,t}$, we use optimality conditions. First, the gradient of Lagrangian wrt to parameter should be zero:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial \\theta_{w,t}} = \\frac{- \\sum_d \\Delta_{w,d,t}}{\\theta_{w,t}} - \\lambda^{\\theta}_{w,t} + \\nu^{\\theta}_t \\overset{\\mathrm{set}}{=} 0 & \\Rightarrow \\\\\n",
    "\\theta_{w,t} = \\frac{- \\sum_d \\Delta_{w,d,t}}{\\lambda^{\\theta}_{w,t} - \\nu^{\\theta}_t} & {}\n",
    "\\end{eqnarray}\n",
    "\n",
    "From complementarity and dual feasibility we can set $\\lambda^{\\theta}_{w,t}=0$.\n",
    "\n",
    "From primal feasibility we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{w} \\theta_{w,t} = 1 & \\Rightarrow \\\\\n",
    "\\frac{\\sum_w \\sum_{d} \\Delta_{w,d,t}}{\\nu^{\\theta}_t} = 1 & \\Rightarrow \\\\\n",
    "\\nu^{\\theta}_t = \\sum_w \\sum_{d} \\Delta_{w,d,t} & \\Rightarrow \\\\\n",
    "\\theta_{w,t} = \\frac{\\sum_d \\Delta_{w,d,t}}{\\sum_w \\sum_d \\Delta_{w,d,t}} & {}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: pLSA with EM Algorithm\n",
    "\n",
    "Denote the _responsibility_ of topic $t$ for the occurences of word $w$ in document $d$ as $\\gamma_{w,d,t}=E[\\Delta_{w,d,t}|\\{p_d\\},\\{\\theta_t\\}]$\n",
    "\n",
    "**Problem 3**: Write out the M-step for the EM algorithm based on the result of Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3 Answer**\n",
    "\n",
    "$$\n",
    "\\hat{p}_{t,d} = \\frac{\\sum_{w=1}^W \\gamma_{w,d,t}}{\\sum_{t=1}^T \\sum_{w=1}^W \\gamma_{w,d,t}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{w,t} = \\frac{\\sum_{d=1}^D \\gamma_{w,d,t}}{\\sum_{w=1}^W \\sum_{d=1}^D \\gamma_{w,d,t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: Show that the E-step for the EM algorithm, i.e., the update $\\gamma_{d_j,t}$ given current set of parameters $\\{p_d\\}$ and $\\{\\theta_t\\}$ is given by\n",
    "\n",
    "$$\n",
    "\\gamma_{w,d,t} = x_{w,d} \\times \\frac{p_{t,d}\\theta_{w,t}}{\\sum_{t'=1}^T p_{t',d}\\theta_{w,t'}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4 Answer**\n",
    "\n",
    "In the EM algorithm we set, and using Bayes Rule\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{w,d,t} & = & E[\\Delta_{w,d,t} | w=W, d=D; p_{t,d}, \\theta_{w,t}] \\\\\n",
    "{} & = & x_{w,d} p(T=t|W=w,D=d; p_{t,d}, \\theta_{w,t}) \\\\\n",
    "{} & = & x_{w,d} \\frac{p(W=w|T=t;p_{t,d},\\theta_{w,t})p(T=t|D=d;p_{t,d})}{p(W=w|D=d;p_{t,d}\\theta_{w,t})} \\\\\n",
    "{} & = & x_{w,d} \\frac{p_{t,d}\\theta_{w,t}}{\\sum_{t'=1}^T p_{t',d}\\theta_{w,t'}}\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Simulating data\n",
    "\n",
    "**Problem 5** Complete the data simulation data in file `topic_lib/simulation.py` See lecture notes on how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from topic_lib.simulation import simulate_data\n",
    "\n",
    "# set simulation parameters\n",
    "num_docs = 20\n",
    "num_words = 100\n",
    "num_topics = 3\n",
    "num_words_per_doc = 20\n",
    "\n",
    "x, sim_delta, sim_p, sim_theta = simulate_data(num_words, num_docs, num_topics, num_words_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a few assertions to check your implementation\n",
    "\n",
    "# check the size of data matrix x\n",
    "assert(x.shape == (num_words, num_docs))\n",
    "\n",
    "# check that the total number of words in a document is correct\n",
    "assert(np.all(np.sum(x, axis=0) == num_words_per_doc))\n",
    "\n",
    "# check the size of simulated latent counts delta\n",
    "assert(sim_delta.shape == (num_words, num_docs, num_topics))\n",
    "\n",
    "# check that the sum of delta across topics equals the counts in data matrix x\n",
    "assert(np.allclose(np.sum(sim_delta, axis=2), x))\n",
    "\n",
    "# check the size of matrix p\n",
    "assert(sim_p.shape == (num_topics, num_docs))\n",
    "\n",
    "# check that p is normalized properly\n",
    "assert(np.allclose(np.sum(sim_p, axis=0), np.ones((num_docs))))\n",
    "\n",
    "# check the size of matrix theta\n",
    "assert(sim_theta.shape == (num_words, num_topics))\n",
    "\n",
    "# check that theta is normalized properly\n",
    "assert(np.allclose(np.sum(sim_theta, axis=0), np.ones((num_topics))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: pLSA using EM\n",
    "\n",
    "Implement pLSA using the EM updates from problems 3 and 4.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- For the pLSA topic model we set out here, the probability of the observed word-document occurences is given by mixture distribution\n",
    "\n",
    "$$\n",
    "Pr(D|\\{p_d\\},\\{\\theta_t\\}) = \n",
    "\\prod_{d=1}^D \\prod_{w=1}^W \\left( \\sum_{t=1}^T p_{t,d} \\theta_{w,t} \\right)^{x_{w,d}}\n",
    "$$\n",
    "\n",
    "- Complete the implementation in file `topic_lib/em.py`\n",
    "\n",
    "- You will need to initialize parameters $\\{p_d\\}$ and $\\{\\theta_t\\}$ (see lecture notes on the Dirichlet distribution)\n",
    "\n",
    "- You will need to test for convergence\n",
    "\n",
    "- You will need to deal with local minima (i.e, use multiple random initial points and choose the model that has largest likelihood).\n",
    "\n",
    "- test your function on the small simulation dataset, i.e., from the data you generate above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_lib.em import plsa_em\n",
    "\n",
    "p, theta, llik = plsa_em(x, num_topics=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n",
      "Important topic in document match rate: 0.75\n",
      "Important topic in document random match rate: 0.3333333333333333\n",
      "\n",
      "Important words in topic 0 match rate: 1.0\n",
      "Important words in topic 1 match rate: 0.6\n",
      "Important words in topic 2 match rate: 0.6\n",
      "Important words in topic random match rate: 0.05\n",
      "\n",
      "Log likelihood for simulation parameters: -1625.8366488092925\n",
      "Log likelhood of estimated parameters: -1488.3715689541239\n",
      "Relative absolute deviation: 0.0845503636271475\n"
     ]
    }
   ],
   "source": [
    "from topic_lib.simulation import compare_topics\n",
    "\n",
    "# let's run a few assertions to check your implementation\n",
    "\n",
    "# check the shape of the p estimate\n",
    "assert(p.shape == (num_topics, num_docs))\n",
    "\n",
    "# make sure the p estimate is properly normalized\n",
    "assert(np.allclose(np.sum(p, axis=0), np.ones((num_docs))))\n",
    "# let's see if you got close to the simulation p\n",
    "# we check it to see if you identify the \"important\" topic in each document\n",
    "# in the simulated p better than random chance\n",
    "topic_assignment = compare_topics(sim_theta, theta)\n",
    "print(topic_assignment)\n",
    "\n",
    "match_rate = np.mean(np.argmax(p[topic_assignment,:],axis=0) == np.argmax(sim_p,axis=0))\n",
    "print(\"Important topic in document match rate: {}\".format(match_rate))\n",
    "print(\"Important topic in document random match rate: {}\\n\".format(1. / num_topics))\n",
    "assert(match_rate > 1. / num_topics)\n",
    "\n",
    "# check the shape of the theta estimate\n",
    "assert(theta.shape == (num_words, num_topics))\n",
    "\n",
    "# make sure the theta estimate is properly normalized\n",
    "assert(np.allclose(np.sum(theta, axis=0), np.ones((num_topics))))\n",
    "\n",
    "# let's see if you get close to the simulation theta\n",
    "# we check it to see if you identify the \"important\" words\n",
    "# in each topic\n",
    "match_rates = np.zeros((num_topics))\n",
    "for t in range(num_topics):\n",
    "    imp_words_sim = np.argsort(sim_theta[:,t])[-5:]\n",
    "    imp_words = np.argsort(theta[:,topic_assignment[t]])[-5:]\n",
    "    match_rates[t] = sum([w in imp_words_sim for w in imp_words]) / 5\n",
    "    print(\"Important words in topic {} match rate: {}\".format(t, match_rates[t]))\n",
    "\n",
    "print(\"Important words in topic random match rate: {}\\n\".format(5. / num_words))\n",
    "assert(np.all(match_rates > 5. / num_words))\n",
    "\n",
    "# let's see how close you got to the log likelihood of the simulation parameters\n",
    "from topic_lib.em import get_loglik\n",
    "sim_ll = get_loglik(x, np.log(sim_p), np.log(sim_theta))\n",
    "est_ll = get_loglik(x, np.log(p), np.log(theta))\n",
    "print(\"Log likelihood for simulation parameters: {}\".format(sim_ll))\n",
    "print(\"Log likelhood of estimated parameters: {}\".format(est_ll))\n",
    "\n",
    "relative_dev = np.abs( sim_ll - est_ll ) / np.abs(sim_ll)\n",
    "print(\"Relative absolute deviation: {}\".format(relative_dev))\n",
    "assert(relative_dev < 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: LDA with Gibbs Sampling\n",
    "\n",
    "Implement Latent Dirichlet Annotation with Gibbs Sampling. See lecture notes for details.\n",
    "\n",
    "Complete the implementation in file `topic_lib/gibbs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_lib.gibbs import lda_gibbs\n",
    "\n",
    "p, theta = lda_gibbs(x, num_topics=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important topic in document match rate: 0.85\n",
      "Important topic in document random match rate: 0.3333333333333333\n",
      "\n",
      "Important words in topic 0 match rate: 1.0\n",
      "Important words in topic 1 match rate: 0.6\n",
      "Important words in topic 2 match rate: 0.4\n",
      "Important words in topic random match rate: 0.05\n",
      "\n",
      "Log likelihood for simulation parameters: -1625.8366488092925\n",
      "Log likelhood of estimated parameters: -1555.2428630679997\n",
      "Relative absolute deviation: 0.043419974443922926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fbf93db57ac8>:41: RuntimeWarning: divide by zero encountered in log\n",
      "  est_ll = get_loglik(x, np.log(p), np.log(theta))\n"
     ]
    }
   ],
   "source": [
    "# let's run a few assertions to check your implementation\n",
    "\n",
    "# check the shape of the p estimate\n",
    "assert(p.shape == (num_topics, num_docs))\n",
    "\n",
    "# make sure the p estimate is properly normalized\n",
    "assert(np.allclose(np.sum(p, axis=0), np.ones((num_docs))))\n",
    "\n",
    "# let's see if you got close to the simulation p\n",
    "# we check it to see if you identify the \"important\" topic in each document\n",
    "# in the simulated p better than random chance\n",
    "topic_assignment = compare_topics(sim_theta, theta)\n",
    "\n",
    "match_rate = np.mean(np.argmax(p[topic_assignment,:],axis=0) == np.argmax(sim_p,axis=0))\n",
    "print(\"Important topic in document match rate: {}\".format(match_rate))\n",
    "print(\"Important topic in document random match rate: {}\\n\".format(1. / num_topics))\n",
    "assert(match_rate > 1. / num_topics)\n",
    "\n",
    "# check the shape of the theta estimate\n",
    "assert(theta.shape == (num_words, num_topics))\n",
    "\n",
    "# make sure the theta estimate is properly normalized\n",
    "assert(np.allclose(np.sum(theta, axis=0), np.ones((num_topics))))\n",
    "\n",
    "# let's see if you get close to the simulation theta\n",
    "# we check it to see if you identify the \"important\" words\n",
    "# in each topic\n",
    "match_rates = np.zeros((num_topics))\n",
    "for t in range(num_topics):\n",
    "    imp_words_sim = np.argsort(sim_theta[:,t])[-5:]\n",
    "    imp_words = np.argsort(theta[:,topic_assignment[t]])[-5:]\n",
    "    match_rates[t] = sum([w in imp_words_sim for w in imp_words]) / 5\n",
    "    print(\"Important words in topic {} match rate: {}\".format(t, match_rates[t]))\n",
    "\n",
    "print(\"Important words in topic random match rate: {}\\n\".format(5. / num_words))\n",
    "assert(np.all(match_rates > 5. / num_words))\n",
    "\n",
    "# let's see how close you got to the log likelihood of the simulation parameters\n",
    "from topic_lib.em import get_loglik\n",
    "sim_ll = get_loglik(x, np.log(sim_p), np.log(sim_theta))\n",
    "est_ll = get_loglik(x, np.log(p), np.log(theta))\n",
    "print(\"Log likelihood for simulation parameters: {}\".format(sim_ll))\n",
    "print(\"Log likelhood of estimated parameters: {}\".format(est_ll))\n",
    "\n",
    "relative_dev = np.abs( sim_ll - est_ll ) / np.abs(sim_ll)\n",
    "print(\"Relative absolute deviation: {}\".format(relative_dev))\n",
    "assert(relative_dev < 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Applying Methods\n",
    "\n",
    "Use your pLSA and LDA implementations to learn topics from the 20 newsgroups dataset. Utilities to\n",
    "download and prepare the dataset is provided in file `topic_lib/newsgroups.py`. To run the\n",
    "`get_docmat` function you will need to install packages gensim and nltk:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "conda install -c anaconda nltk\n",
    "```\n",
    "\n",
    "Compare topics learned from pLSA and LDA with number of topics $T=8$\n",
    "\n",
    "To perform the comparison, print the top 5 words if each topic for each model (pLSA and LDA) (using function `get_topic_words` from file `topic_lib/newsgroups.py`.\n",
    "\n",
    "(a) Do the topics they each return sensible?  \n",
    "(b) Do the topics for one method make more sense than the other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved docmat file. Loading...\n",
      "(1590, 1769)\n"
     ]
    }
   ],
   "source": [
    "from topic_lib.newsgroups import get_docmat\n",
    "\n",
    "newsgroups_mat = get_docmat()\n",
    "print(newsgroups_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 of 3\n",
      "It: 0, loglik: -717479.41727, old_q: -902172.52273, new_q: -864775.98201\n",
      "It: 1, loglik: -714163.81412, old_q: -867626.72596, new_q: -865866.31294\n",
      "It: 2, loglik: -711078.11246, old_q: -863412.11736, new_q: -861879.41889\n",
      "It: 3, loglik: -707608.77947, old_q: -855862.25964, new_q: -854166.56771\n",
      "It: 4, loglik: -703586.16845, old_q: -845339.50685, new_q: -843357.10803\n",
      "It: 5, loglik: -699097.24887, old_q: -832448.67954, new_q: -830192.72407\n",
      "It: 6, loglik: -694415.35528, old_q: -818171.37009, new_q: -815767.72407\n",
      "It: 7, loglik: -689817.07790, old_q: -803724.08713, new_q: -801332.75704\n",
      "It: 8, loglik: -685453.39601, old_q: -790057.53801, new_q: -787775.97591\n",
      "It: 9, loglik: -681383.88885, old_q: -777595.64560, new_q: -775461.81932\n",
      "It: 10, loglik: -677654.55154, old_q: -766385.79923, new_q: -764421.79584\n",
      "It: 11, loglik: -674291.28610, old_q: -756392.24742, new_q: -754617.29374\n",
      "It: 12, loglik: -671268.41821, old_q: -747561.92667, new_q: -745968.94124\n",
      "It: 13, loglik: -668558.33424, old_q: -739748.95346, new_q: -738313.60799\n",
      "It: 14, loglik: -666202.27983, old_q: -732830.38314, new_q: -731568.70075\n",
      "It: 15, loglik: -664216.24976, old_q: -726846.38603, new_q: -725779.26306\n",
      "It: 16, loglik: -662555.43698, old_q: -721772.66233, new_q: -720881.71841\n",
      "It: 17, loglik: -661159.80987, old_q: -717490.65927, new_q: -716743.87862\n",
      "It: 18, loglik: -659979.03089, old_q: -713858.29759, new_q: -713228.38767\n",
      "It: 19, loglik: -658967.75296, old_q: -710750.36550, new_q: -710213.30560\n",
      "It: 20, loglik: -658091.73648, old_q: -708056.71006, new_q: -707592.07621\n",
      "Run 1 of 3\n",
      "It: 0, loglik: -717281.29017, old_q: -902927.52972, new_q: -866032.60528\n",
      "It: 1, loglik: -713859.22089, old_q: -868449.22890, new_q: -866647.08919\n",
      "It: 2, loglik: -710544.94533, old_q: -863706.81023, new_q: -862076.09758\n",
      "It: 3, loglik: -706675.59506, old_q: -855394.22811, new_q: -853519.50486\n",
      "It: 4, loglik: -702041.85476, old_q: -843779.19473, new_q: -841510.83242\n",
      "It: 5, loglik: -696745.40403, old_q: -829437.06310, new_q: -826786.23596\n",
      "It: 6, loglik: -691151.43246, old_q: -813436.16111, new_q: -810564.22191\n",
      "It: 7, loglik: -685714.24187, old_q: -797188.48175, new_q: -794337.93508\n",
      "It: 8, loglik: -680779.55484, old_q: -781948.39654, new_q: -779318.78535\n",
      "It: 9, loglik: -676579.15780, old_q: -768428.06028, new_q: -766149.38666\n",
      "It: 10, loglik: -673223.36765, old_q: -756971.60500, new_q: -755128.37826\n",
      "It: 11, loglik: -670614.63617, old_q: -747670.47110, new_q: -746239.22777\n",
      "It: 12, loglik: -668560.92052, old_q: -740224.68911, new_q: -739106.38544\n",
      "It: 13, loglik: -666899.87358, old_q: -734186.28854, new_q: -733287.78803\n",
      "It: 14, loglik: -665524.76897, old_q: -729189.17500, new_q: -728450.27791\n",
      "It: 15, loglik: -664352.67821, old_q: -724990.25384, new_q: -724366.08394\n",
      "It: 16, loglik: -663325.33979, old_q: -721395.63256, new_q: -720851.44322\n",
      "It: 17, loglik: -662414.14976, old_q: -718262.73891, new_q: -717780.97168\n",
      "It: 18, loglik: -661605.34766, old_q: -715511.46668, new_q: -715083.54619\n",
      "It: 19, loglik: -660879.07601, old_q: -713090.00112, new_q: -712708.94581\n",
      "It: 20, loglik: -660212.33597, old_q: -710932.75699, new_q: -710583.85518\n",
      "Run 2 of 3\n",
      "It: 0, loglik: -717078.58646, old_q: -903138.49106, new_q: -865531.32918\n",
      "It: 1, loglik: -713632.11586, old_q: -867836.15457, new_q: -866016.55170\n",
      "It: 2, loglik: -710348.39303, old_q: -862994.59419, new_q: -861368.25254\n",
      "It: 3, loglik: -706630.30852, old_q: -854710.80359, new_q: -852890.54158\n",
      "It: 4, loglik: -702342.29818, old_q: -843390.75467, new_q: -841270.95246\n",
      "It: 5, loglik: -697569.18775, old_q: -829721.58833, new_q: -827324.60806\n",
      "It: 6, loglik: -692549.11523, old_q: -814632.06707, new_q: -812061.79518\n",
      "It: 7, loglik: -687617.46991, old_q: -799232.13872, new_q: -796657.11576\n",
      "It: 8, loglik: -683071.19029, old_q: -784660.35397, new_q: -782252.51165\n",
      "It: 9, loglik: -679042.64293, old_q: -771699.46618, new_q: -769554.74131\n",
      "It: 10, loglik: -675538.00660, old_q: -760519.73716, new_q: -758648.02341\n",
      "It: 11, loglik: -672541.49970, old_q: -750960.28689, new_q: -749352.71669\n",
      "It: 12, loglik: -670032.76104, old_q: -742832.72677, new_q: -741479.18172\n",
      "It: 13, loglik: -667978.97593, old_q: -735973.25583, new_q: -734858.68833\n",
      "It: 14, loglik: -666328.83182, old_q: -730249.05515, new_q: -729351.24266\n",
      "It: 15, loglik: -664995.08068, old_q: -725532.77785, new_q: -724812.43872\n",
      "It: 16, loglik: -663888.61170, old_q: -721625.85850, new_q: -721031.91330\n",
      "It: 17, loglik: -662948.28990, old_q: -718326.78366, new_q: -717826.14598\n",
      "It: 18, loglik: -662129.79937, old_q: -715495.23885, new_q: -715061.93437\n",
      "It: 19, loglik: -661404.45803, old_q: -713032.88397, new_q: -712650.97472\n",
      "It: 20, loglik: -660754.37277, old_q: -710866.99843, new_q: -710525.11282\n"
     ]
    }
   ],
   "source": [
    "# run each of the methods \n",
    "# (modify num_restarts and max_iter here)\n",
    "em_p, em_theta, _ = plsa_em(newsgroups_mat, num_topics=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, log_likelihood: -719062.0456314915\n",
      "Iteration 10, log_likelihood: -696587.6542194713\n",
      "Iteration 20, log_likelihood: -675002.659430214\n",
      "Iteration 30, log_likelihood: -670130.9126155886\n",
      "Iteration 40, log_likelihood: -668402.0192358766\n",
      "Iteration 50, log_likelihood: -667357.0695054223\n",
      "Iteration 60, log_likelihood: -667036.1813403091\n",
      "Iteration 70, log_likelihood: -666909.2768358274\n",
      "Iteration 80, log_likelihood: -666824.9369911373\n",
      "Iteration 90, log_likelihood: -666764.8225244443\n",
      "Iteration 100, log_likelihood: -666754.6135151176\n",
      "Iteration 110, log_likelihood: -666638.3832313546\n",
      "Iteration 120, log_likelihood: -666821.3457409298\n",
      "Iteration 130, log_likelihood: -666717.355475865\n",
      "Iteration 140, log_likelihood: -666552.8770153266\n",
      "Iteration 150, log_likelihood: -666553.0518274568\n",
      "Iteration 160, log_likelihood: -666682.4288232364\n",
      "Iteration 170, log_likelihood: -666355.8014198397\n",
      "Iteration 180, log_likelihood: -666390.5686715703\n",
      "Iteration 190, log_likelihood: -666093.8238488914\n"
     ]
    }
   ],
   "source": [
    "# modify num_rounds parameter here\n",
    "gibbs_p, gibbs_theta = lda_gibbs(newsgroups_mat, num_topics=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1590 unique tokens: ['acceler', 'adapt', 'answer', 'base', 'brave']...)\n",
      "['lose', 'cub', 'brave', 'score', 'pitch']\n",
      "['netcom', 'major', 'alomar', 'roger', 'player']\n",
      "['power', 'austin', 'nasa', 'uiuc', 'david']\n",
      "['indiana', 'type', 'manag', 'sound', 'keyboard']\n",
      "['machin', 'simm', 'monitor', 'card', 'drive']\n",
      "['medic', 'pain', 'diseas', 'patient', 'doctor']\n",
      "['dyer', 'state', 'research', 'health', 'studi']\n",
      "['pittsburgh', 'food', 'gordon', 'bank', 'pitt']\n"
     ]
    }
   ],
   "source": [
    "from topic_lib.newsgroups import print_important_words\n",
    "\n",
    "# print important words from EM estimate\n",
    "print_important_words(em_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1590 unique tokens: ['acceler', 'adapt', 'answer', 'base', 'brave']...)\n",
      "['center', 'medic', 'state', 'research', 'health']\n",
      "['univ', 'pittsburgh', 'gordon', 'bank', 'pitt']\n",
      "['list', 'berkeley', 'water', 'type', 'keyboard']\n",
      "['scsi', 'simm', 'monitor', 'card', 'drive']\n",
      "['major', 'roger', 'averag', 'david', 'player']\n",
      "['experi', 'sensit', 'nasa', 'dyer', 'food']\n",
      "['cub', 'season', 'lose', 'score', 'pitch']\n",
      "['treatment', 'diseas', 'caus', 'patient', 'doctor']\n"
     ]
    }
   ],
   "source": [
    "# print important words from Gibbs estimates\n",
    "print_important_words(gibbs_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer questions (a) and (b) here**\n",
    "\n",
    "(a) Both methods return for the most part sensible topics related to medicine, sports and computer hardware. Some topics in both cases include individual words that may not be relevant to that topic (e.g, topic 4 for em). \n",
    "\n",
    "(b) Topics from both methods are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
